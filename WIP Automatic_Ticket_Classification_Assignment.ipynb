{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhR-ZUkwJrFn"
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "You need to build a model that is able to classify customer complaints based on the products/services. By doing so, you can segregate these tickets into their relevant categories and, therefore, help in the quick resolution of the issue.\n",
    "\n",
    "You will be doing topic modelling on the <b>.json</b> data provided by the company. Since this data is not labelled, you need to apply NMF to analyse patterns and classify tickets into the following five clusters based on their products/services:\n",
    "\n",
    "* Credit card / Prepaid card\n",
    "\n",
    "* Bank account services\n",
    "\n",
    "* Theft/Dispute reporting\n",
    "\n",
    "* Mortgages/loans\n",
    "\n",
    "* Others\n",
    "\n",
    "\n",
    "With the help of topic modelling, you will be able to map each ticket onto its respective department/category. You can then use this data to train any supervised model such as logistic regression, decision tree or random forest. Using this trained model, you can classify any new customer complaint support ticket into its relevant department."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcgXVNyaLUFS"
   },
   "source": [
    "## Pipelines that needs to be performed:\n",
    "\n",
    "You need to perform the following eight major tasks to complete the assignment:\n",
    "\n",
    "1.  Data loading\n",
    "\n",
    "2. Text preprocessing\n",
    "\n",
    "3. Exploratory data analysis (EDA)\n",
    "\n",
    "4. Feature extraction\n",
    "\n",
    "5. Topic modelling\n",
    "\n",
    "6. Model building using supervised learning\n",
    "\n",
    "7. Model training and evaluation\n",
    "\n",
    "8. Model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuLFIymAL58u"
   },
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O-Q9pqrcJrFr"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, string\n",
    "import en_core_web_sm\n",
    "from tqdm import tqdm\n",
    "nlp = en_core_web_sm.load()\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from plotly.offline import plot\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the display properties of pandas to max\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtRLCsNVJrFt"
   },
   "source": [
    "## Loading the data\n",
    "\n",
    "The data is in JSON format and we need to convert it to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "puVzIf_iJrFt"
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('complaints-2021-05-14_08_16.json')\n",
    "\n",
    "# returns JSON object as\n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "df=pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_index</th>\n",
       "      <th>_type</th>\n",
       "      <th>_id</th>\n",
       "      <th>_score</th>\n",
       "      <th>_source.tags</th>\n",
       "      <th>_source.zip_code</th>\n",
       "      <th>_source.complaint_id</th>\n",
       "      <th>_source.issue</th>\n",
       "      <th>_source.date_received</th>\n",
       "      <th>_source.state</th>\n",
       "      <th>...</th>\n",
       "      <th>_source.company_response</th>\n",
       "      <th>_source.company</th>\n",
       "      <th>_source.submitted_via</th>\n",
       "      <th>_source.date_sent_to_company</th>\n",
       "      <th>_source.company_public_response</th>\n",
       "      <th>_source.sub_product</th>\n",
       "      <th>_source.timely</th>\n",
       "      <th>_source.complaint_what_happened</th>\n",
       "      <th>_source.sub_issue</th>\n",
       "      <th>_source.consumer_consent_provided</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>complaint-public-v2</td>\n",
       "      <td>complaint</td>\n",
       "      <td>3211475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>90301</td>\n",
       "      <td>3211475</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>2019-04-13T12:00:00-05:00</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>JPMORGAN CHASE &amp; CO.</td>\n",
       "      <td>Web</td>\n",
       "      <td>2019-04-13T12:00:00-05:00</td>\n",
       "      <td>None</td>\n",
       "      <td>Credit card debt</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Debt is not yours</td>\n",
       "      <td>Consent not provided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>complaint-public-v2</td>\n",
       "      <td>complaint</td>\n",
       "      <td>3229299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Servicemember</td>\n",
       "      <td>319XX</td>\n",
       "      <td>3229299</td>\n",
       "      <td>Written notification about debt</td>\n",
       "      <td>2019-05-01T12:00:00-05:00</td>\n",
       "      <td>GA</td>\n",
       "      <td>...</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>JPMORGAN CHASE &amp; CO.</td>\n",
       "      <td>Web</td>\n",
       "      <td>2019-05-01T12:00:00-05:00</td>\n",
       "      <td>None</td>\n",
       "      <td>Credit card debt</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Good morning my name is XXXX XXXX and I appreciate it if you could help me put a stop to Chase B...</td>\n",
       "      <td>Didn't receive enough information to verify debt</td>\n",
       "      <td>Consent provided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>complaint-public-v2</td>\n",
       "      <td>complaint</td>\n",
       "      <td>3199379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>77069</td>\n",
       "      <td>3199379</td>\n",
       "      <td>Other features, terms, or problems</td>\n",
       "      <td>2019-04-02T12:00:00-05:00</td>\n",
       "      <td>TX</td>\n",
       "      <td>...</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>JPMORGAN CHASE &amp; CO.</td>\n",
       "      <td>Web</td>\n",
       "      <td>2019-04-02T12:00:00-05:00</td>\n",
       "      <td>None</td>\n",
       "      <td>General-purpose credit card or charge card</td>\n",
       "      <td>Yes</td>\n",
       "      <td>I upgraded my XXXX XXXX card in XX/XX/2018 and was told by the agent who did the upgrade my anni...</td>\n",
       "      <td>Problem with rewards from credit card</td>\n",
       "      <td>Consent provided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>complaint-public-v2</td>\n",
       "      <td>complaint</td>\n",
       "      <td>2673060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>48066</td>\n",
       "      <td>2673060</td>\n",
       "      <td>Trouble during payment process</td>\n",
       "      <td>2017-09-13T12:00:00-05:00</td>\n",
       "      <td>MI</td>\n",
       "      <td>...</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>JPMORGAN CHASE &amp; CO.</td>\n",
       "      <td>Web</td>\n",
       "      <td>2017-09-14T12:00:00-05:00</td>\n",
       "      <td>None</td>\n",
       "      <td>Conventional home mortgage</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>Consent not provided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>complaint-public-v2</td>\n",
       "      <td>complaint</td>\n",
       "      <td>3203545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>10473</td>\n",
       "      <td>3203545</td>\n",
       "      <td>Fees or interest</td>\n",
       "      <td>2019-04-05T12:00:00-05:00</td>\n",
       "      <td>NY</td>\n",
       "      <td>...</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>JPMORGAN CHASE &amp; CO.</td>\n",
       "      <td>Referral</td>\n",
       "      <td>2019-04-05T12:00:00-05:00</td>\n",
       "      <td>None</td>\n",
       "      <td>General-purpose credit card or charge card</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Charged too much interest</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                _index      _type      _id  _score   _source.tags  \\\n",
       "0  complaint-public-v2  complaint  3211475     0.0           None   \n",
       "1  complaint-public-v2  complaint  3229299     0.0  Servicemember   \n",
       "2  complaint-public-v2  complaint  3199379     0.0           None   \n",
       "3  complaint-public-v2  complaint  2673060     0.0           None   \n",
       "4  complaint-public-v2  complaint  3203545     0.0           None   \n",
       "\n",
       "  _source.zip_code _source.complaint_id                       _source.issue  \\\n",
       "0            90301              3211475   Attempts to collect debt not owed   \n",
       "1            319XX              3229299     Written notification about debt   \n",
       "2            77069              3199379  Other features, terms, or problems   \n",
       "3            48066              2673060      Trouble during payment process   \n",
       "4            10473              3203545                    Fees or interest   \n",
       "\n",
       "       _source.date_received _source.state  ... _source.company_response  \\\n",
       "0  2019-04-13T12:00:00-05:00            CA  ...  Closed with explanation   \n",
       "1  2019-05-01T12:00:00-05:00            GA  ...  Closed with explanation   \n",
       "2  2019-04-02T12:00:00-05:00            TX  ...  Closed with explanation   \n",
       "3  2017-09-13T12:00:00-05:00            MI  ...  Closed with explanation   \n",
       "4  2019-04-05T12:00:00-05:00            NY  ...  Closed with explanation   \n",
       "\n",
       "        _source.company _source.submitted_via _source.date_sent_to_company  \\\n",
       "0  JPMORGAN CHASE & CO.                   Web    2019-04-13T12:00:00-05:00   \n",
       "1  JPMORGAN CHASE & CO.                   Web    2019-05-01T12:00:00-05:00   \n",
       "2  JPMORGAN CHASE & CO.                   Web    2019-04-02T12:00:00-05:00   \n",
       "3  JPMORGAN CHASE & CO.                   Web    2017-09-14T12:00:00-05:00   \n",
       "4  JPMORGAN CHASE & CO.              Referral    2019-04-05T12:00:00-05:00   \n",
       "\n",
       "  _source.company_public_response                         _source.sub_product  \\\n",
       "0                            None                            Credit card debt   \n",
       "1                            None                            Credit card debt   \n",
       "2                            None  General-purpose credit card or charge card   \n",
       "3                            None                  Conventional home mortgage   \n",
       "4                            None  General-purpose credit card or charge card   \n",
       "\n",
       "  _source.timely  \\\n",
       "0            Yes   \n",
       "1            Yes   \n",
       "2            Yes   \n",
       "3            Yes   \n",
       "4            Yes   \n",
       "\n",
       "                                                                       _source.complaint_what_happened  \\\n",
       "0                                                                                                        \n",
       "1  Good morning my name is XXXX XXXX and I appreciate it if you could help me put a stop to Chase B...   \n",
       "2  I upgraded my XXXX XXXX card in XX/XX/2018 and was told by the agent who did the upgrade my anni...   \n",
       "3                                                                                                        \n",
       "4                                                                                                        \n",
       "\n",
       "                                  _source.sub_issue  \\\n",
       "0                                 Debt is not yours   \n",
       "1  Didn't receive enough information to verify debt   \n",
       "2             Problem with rewards from credit card   \n",
       "3                                              None   \n",
       "4                         Charged too much interest   \n",
       "\n",
       "  _source.consumer_consent_provided  \n",
       "0              Consent not provided  \n",
       "1                  Consent provided  \n",
       "2                  Consent provided  \n",
       "3              Consent not provided  \n",
       "4                               N/A  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78313, 22)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xYpH-sAJrFu"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Lf8ufHH5JrFu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 78313 entries, 0 to 78312\n",
      "Data columns (total 22 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   _index                             78313 non-null  object \n",
      " 1   _type                              78313 non-null  object \n",
      " 2   _id                                78313 non-null  object \n",
      " 3   _score                             78313 non-null  float64\n",
      " 4   _source.tags                       10900 non-null  object \n",
      " 5   _source.zip_code                   71556 non-null  object \n",
      " 6   _source.complaint_id               78313 non-null  object \n",
      " 7   _source.issue                      78313 non-null  object \n",
      " 8   _source.date_received              78313 non-null  object \n",
      " 9   _source.state                      76322 non-null  object \n",
      " 10  _source.consumer_disputed          78313 non-null  object \n",
      " 11  _source.product                    78313 non-null  object \n",
      " 12  _source.company_response           78313 non-null  object \n",
      " 13  _source.company                    78313 non-null  object \n",
      " 14  _source.submitted_via              78313 non-null  object \n",
      " 15  _source.date_sent_to_company       78313 non-null  object \n",
      " 16  _source.company_public_response    4 non-null      object \n",
      " 17  _source.sub_product                67742 non-null  object \n",
      " 18  _source.timely                     78313 non-null  object \n",
      " 19  _source.complaint_what_happened    78313 non-null  object \n",
      " 20  _source.sub_issue                  32016 non-null  object \n",
      " 21  _source.consumer_consent_provided  77305 non-null  object \n",
      "dtypes: float64(1), object(21)\n",
      "memory usage: 13.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataframe to understand the given data.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Dwcty-wmJrFw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_index', '_type', '_id', '_score', '_source.tags', '_source.zip_code',\n",
       "       '_source.complaint_id', '_source.issue', '_source.date_received',\n",
       "       '_source.state', '_source.consumer_disputed', '_source.product',\n",
       "       '_source.company_response', '_source.company', '_source.submitted_via',\n",
       "       '_source.date_sent_to_company', '_source.company_public_response',\n",
       "       '_source.sub_product', '_source.timely',\n",
       "       '_source.complaint_what_happened', '_source.sub_issue',\n",
       "       '_source.consumer_consent_provided'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FYCtKXD1JrFw"
   },
   "outputs": [],
   "source": [
    "#Assign new column names\n",
    "df.rename(columns={'_index':'index',\n",
    "  '_type':'type',\n",
    "  '_id':'id',\n",
    "  '_score':'score',\n",
    "  '_source.tags':'tags',\n",
    "  '_source.zip_code':'zip_code',\n",
    " '_source.complaint_id':'complaint_id',\n",
    " '_source.issue':'issue',\n",
    " '_source.date_received':'date_received',\n",
    " '_source.state':'state',\n",
    " '_source.consumer_disputed':'consumer_disputed',\n",
    " '_source.product':'product',\n",
    " '_source.company_response':'company_response',\n",
    " '_source.company':'company',\n",
    " '_source.submitted_via':'submitted_via',\n",
    " '_source.date_sent_to_company':'date_sent_to_company',\n",
    " '_source.company_public_response':'company_public_response',\n",
    " '_source.sub_product':'sub_product',\n",
    " '_source.timely':'timely',\n",
    " '_source.complaint_what_happened':'complaint_what_happened',\n",
    " '_source.sub_issue':'sub_issue',\n",
    " '_source.consumer_consent_provided':'consumer_consent_provided'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                            0\n",
       "type                             0\n",
       "id                               0\n",
       "score                            0\n",
       "tags                         67413\n",
       "zip_code                      6757\n",
       "complaint_id                     0\n",
       "issue                            0\n",
       "date_received                    0\n",
       "state                         1991\n",
       "consumer_disputed                0\n",
       "product                          0\n",
       "company_response                 0\n",
       "company                          0\n",
       "submitted_via                    0\n",
       "date_sent_to_company             0\n",
       "company_public_response      78309\n",
       "sub_product                  10571\n",
       "timely                           0\n",
       "complaint_what_happened          0\n",
       "sub_issue                    46297\n",
       "consumer_consent_provided     1008\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking nulls values in each columns\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking rows where complaint_what_happened is blank\n",
    "#df[df['complaint_what_happened']=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "grQUPFL5JrFx"
   },
   "outputs": [],
   "source": [
    "#Assign nan in place of blanks in the complaints column\n",
    "df['complaint_what_happened'].replace(r'^\\s*$', np.nan, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['complaint_what_happened'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                            0\n",
       "type                             0\n",
       "id                               0\n",
       "score                            0\n",
       "tags                         67413\n",
       "zip_code                      6757\n",
       "complaint_id                     0\n",
       "issue                            0\n",
       "date_received                    0\n",
       "state                         1991\n",
       "consumer_disputed                0\n",
       "product                          0\n",
       "company_response                 0\n",
       "company                          0\n",
       "submitted_via                    0\n",
       "date_sent_to_company             0\n",
       "company_public_response      78309\n",
       "sub_product                  10571\n",
       "timely                           0\n",
       "complaint_what_happened      57241\n",
       "sub_issue                    46297\n",
       "consumer_consent_provided     1008\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Jfxd8VSmJrFy"
   },
   "outputs": [],
   "source": [
    "#Remove all rows where complaints column is nan\n",
    "df.dropna(subset=['complaint_what_happened'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                            0\n",
       "type                             0\n",
       "id                               0\n",
       "score                            0\n",
       "tags                         17256\n",
       "zip_code                      4645\n",
       "complaint_id                     0\n",
       "issue                            0\n",
       "date_received                    0\n",
       "state                          143\n",
       "consumer_disputed                0\n",
       "product                          0\n",
       "company_response                 0\n",
       "company                          0\n",
       "submitted_via                    0\n",
       "date_sent_to_company             0\n",
       "company_public_response      21070\n",
       "sub_product                   2109\n",
       "timely                           0\n",
       "complaint_what_happened          0\n",
       "sub_issue                     8176\n",
       "consumer_consent_provided        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L944HZpsJrFy"
   },
   "source": [
    "## Prepare the text for topic modeling\n",
    "\n",
    "Once you have removed all the blank complaints, you need to:\n",
    "\n",
    "* Make the text lowercase\n",
    "* Remove text in square brackets\n",
    "* Remove punctuation\n",
    "* Remove words containing numbers\n",
    "\n",
    "\n",
    "Once you have done these cleaning operations you need to perform the following:\n",
    "* Lemmatize the texts\n",
    "* Extract the POS tags of the lemmatized text and remove all the words which have tags other than NN[tag == \"NN\"].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef find_pattern(text, patterns):\\n    if re.search(patterns, text):\\n        return re.search(patterns, text)\\n    else:\\n        return 'Not Found!'\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def find_pattern(text, patterns):\n",
    "    if re.search(patterns, text):\n",
    "        return re.search(patterns, text)\n",
    "    else:\n",
    "        return 'Not Found!'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pattern = '\\[\\w+\\]'\n",
    "#pattern  = '\\[[^\\]]+\\]'\n",
    "#pattern  = '[^\\w\\s]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['comp'] = df['complaint_what_happened'].apply(lambda x: find_pattern(x,pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['comp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = df['complaint_what_happened'][df['comp'] != 'Not Found!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in text.index:\\n    print(i,'\\n')\\n    print(df['complaint_what_happened'][i])\\n    print('*'*100)\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in text.index:\n",
    "    print(i,'\\n')\n",
    "    print(df['complaint_what_happened'][i])\n",
    "    print('*'*100)\n",
    "'''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #Making text Lower case \n",
    "    text = text.lower()\n",
    "\n",
    "    #Removing text in Square brackets []\n",
    "    text = re.sub(r'\\[[^\\]]+\\]',' ',text)\n",
    "    #Removing Puntuations\n",
    "    text = re.sub(r'[^\\w\\s/]',' ',text)\n",
    "\n",
    "    # words containg numbers \n",
    "    text = re.sub(r'\\b[a-zA-Z]*\\d+[a-zA-Z]+\\b|\\b[a-zA-Z]+\\d+\\w*\\b',' ',text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef clean_texts1(text):\\n    #Make the text lowercase\\n    text=text.lower()\\n\\n    #Remove text in square brackets\\n    text=re.sub(r'','',text)\\n\\n    #Remove punctuation\\n    text=re.sub(r'[%s]%re.escape(string.punctuation)','',text)\\n\\n    #Remove words containing numbers\\n    text = re.sub(r'\\\\w*\\\\d\\\\w*', '', text)\\n\\n    return text\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def clean_texts1(text):\n",
    "    #Make the text lowercase\n",
    "    text=text.lower()\n",
    "\n",
    "    #Remove text in square brackets\n",
    "    text=re.sub(r'','',text)\n",
    "\n",
    "    #Remove punctuation\n",
    "    text=re.sub(r'[%s]%re.escape(string.punctuation)','',text)\n",
    "\n",
    "    #Remove words containing numbers\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "\n",
    "    return text\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef clean_text2(text):\\n  text=text.lower()  #convert to lower case\\n  text=re.sub(r'^\\\\[[\\\\w\\\\s]\\\\]+$',' ',text) #Remove text in square brackets\\n  text=re.sub(r'[^\\\\w\\\\s]',' ',text) #Remove punctuation\\n  text=re.sub(r'^[a-zA-Z]\\\\d+\\\\w*$',' ',text) #Remove words with numbers\\n  return text\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def clean_text2(text):\n",
    "  text=text.lower()  #convert to lower case\n",
    "  text=re.sub(r'^\\[[\\w\\s]\\]+$',' ',text) #Remove text in square brackets\n",
    "  text=re.sub(r'[^\\w\\s]',' ',text) #Remove punctuation\n",
    "  text=re.sub(r'^[a-zA-Z]\\d+\\w*$',' ',text) #Remove words with numbers\n",
    "  return text\n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|██████████████████████████████████████████████████████████████████████████| 21072/21072 [00:04<00:00, 4881.77it/s]"
     ]
    }
   ],
   "source": [
    "df['complaint_what_happened_cleaned'] = df['complaint_what_happened'].progress_apply(lambda x : clean_text(x))\n",
    "#df['complaint_what_happened_cleaned1'] = df['complaint_what_happened'].apply(lambda x : clean_texts1(x))\n",
    "#df['complaint_what_happened_cleaned2'] = df['complaint_what_happened'].apply(lambda x : clean_text2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint_what_happened</th>\n",
       "      <th>complaint_what_happened_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good morning my name is XXXX XXXX and I appreciate it if you could help me put a stop to Chase B...</td>\n",
       "      <td>good morning my name is xxxx xxxx and i appreciate it if you could help me put a stop to chase b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I upgraded my XXXX XXXX card in XX/XX/2018 and was told by the agent who did the upgrade my anni...</td>\n",
       "      <td>i upgraded my xxxx xxxx card in xx/xx/2018 and was told by the agent who did the upgrade my anni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chase Card was reported on XX/XX/2019. However, fraudulent application have been submitted my id...</td>\n",
       "      <td>chase card was reported on xx/xx/2019  however  fraudulent application have been submitted my id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>On XX/XX/2018, while trying to book a XXXX  XXXX  ticket, I came across an offer for {$300.00} t...</td>\n",
       "      <td>on xx/xx/2018  while trying to book a xxxx  xxxx  ticket  i came across an offer for   300 00  t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>my grand son give me check for {$1600.00} i deposit it into my chase account after fund clear my...</td>\n",
       "      <td>my grand son give me check for   1600 00  i deposit it into my chase account after fund clear my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78303</th>\n",
       "      <td>After being a Chase Card customer for well over a decade, was offered multiple solicitations for...</td>\n",
       "      <td>after being a chase card customer for well over a decade  was offered multiple solicitations for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78309</th>\n",
       "      <td>On Wednesday, XX/XX/XXXX I called Chas, my XXXX XXXX Visa Credit Card provider, and asked how to...</td>\n",
       "      <td>on wednesday  xx/xx/xxxx i called chas  my xxxx xxxx visa credit card provider  and asked how to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78310</th>\n",
       "      <td>I am not familiar with XXXX pay and did not understand the great risk this provides to consumers...</td>\n",
       "      <td>i am not familiar with xxxx pay and did not understand the great risk this provides to consumers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78311</th>\n",
       "      <td>I have had flawless credit for 30 yrs. I've had Chase credit cards, \" Chase Freedom '' specifica...</td>\n",
       "      <td>i have had flawless credit for 30 yrs  i ve had chase credit cards    chase freedom    specifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78312</th>\n",
       "      <td>Roughly 10+ years ago I closed out my accounts with JP Morgan Chase Bank XXXX in order to close ...</td>\n",
       "      <td>roughly 10  years ago i closed out my accounts with jp morgan chase bank xxxx in order to close ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21072 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   complaint_what_happened  \\\n",
       "1      Good morning my name is XXXX XXXX and I appreciate it if you could help me put a stop to Chase B...   \n",
       "2      I upgraded my XXXX XXXX card in XX/XX/2018 and was told by the agent who did the upgrade my anni...   \n",
       "10     Chase Card was reported on XX/XX/2019. However, fraudulent application have been submitted my id...   \n",
       "11     On XX/XX/2018, while trying to book a XXXX  XXXX  ticket, I came across an offer for {$300.00} t...   \n",
       "14     my grand son give me check for {$1600.00} i deposit it into my chase account after fund clear my...   \n",
       "...                                                                                                    ...   \n",
       "78303  After being a Chase Card customer for well over a decade, was offered multiple solicitations for...   \n",
       "78309  On Wednesday, XX/XX/XXXX I called Chas, my XXXX XXXX Visa Credit Card provider, and asked how to...   \n",
       "78310  I am not familiar with XXXX pay and did not understand the great risk this provides to consumers...   \n",
       "78311  I have had flawless credit for 30 yrs. I've had Chase credit cards, \" Chase Freedom '' specifica...   \n",
       "78312  Roughly 10+ years ago I closed out my accounts with JP Morgan Chase Bank XXXX in order to close ...   \n",
       "\n",
       "                                                                           complaint_what_happened_cleaned  \n",
       "1      good morning my name is xxxx xxxx and i appreciate it if you could help me put a stop to chase b...  \n",
       "2      i upgraded my xxxx xxxx card in xx/xx/2018 and was told by the agent who did the upgrade my anni...  \n",
       "10     chase card was reported on xx/xx/2019  however  fraudulent application have been submitted my id...  \n",
       "11     on xx/xx/2018  while trying to book a xxxx  xxxx  ticket  i came across an offer for   300 00  t...  \n",
       "14     my grand son give me check for   1600 00  i deposit it into my chase account after fund clear my...  \n",
       "...                                                                                                    ...  \n",
       "78303  after being a chase card customer for well over a decade  was offered multiple solicitations for...  \n",
       "78309  on wednesday  xx/xx/xxxx i called chas  my xxxx xxxx visa credit card provider  and asked how to...  \n",
       "78310  i am not familiar with xxxx pay and did not understand the great risk this provides to consumers...  \n",
       "78311  i have had flawless credit for 30 yrs  i ve had chase credit cards    chase freedom    specifica...  \n",
       "78312  roughly 10  years ago i closed out my accounts with jp morgan chase bank xxxx in order to close ...  \n",
       "\n",
       "[21072 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['complaint_what_happened','complaint_what_happened_cleaned']]\n",
    "#df[['complaint_what_happened','complaint_what_happened_cleaned','complaint_what_happened_cleaned1','complaint_what_happened_cleaned2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "qm7SjjSkJrFz"
   },
   "outputs": [],
   "source": [
    "# Write your function here to clean the text and remove all the unnecessary elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "zgOu8t8HJrFz"
   },
   "outputs": [],
   "source": [
    "#Write your function to Lemmatize the texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = df['complaint_what_happened_cleaned'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwords = []\\nlemm = []\\nfor word in text.split():\\n    words.append(word)\\n    lemma = lemmatizer.lemmatize(word)\\n    lemm.append(lemma)\\n    #df1 = pd.DataFrame(data={'Word': word, 'lemmatized': lemma})\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "words = []\n",
    "lemm = []\n",
    "for word in text.split():\n",
    "    words.append(word)\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    lemm.append(lemma)\n",
    "    #df1 = pd.DataFrame(data={'Word': word, 'lemmatized': lemma})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf1 = pd.DataFrame(data={'Word': words, 'lemmatized': lemm})\\ndf1[df1['Word'] != df1['lemmatized']]\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df1 = pd.DataFrame(data={'Word': words, 'lemmatized': lemm})\n",
    "df1[df1['Word'] != df1['lemmatized']]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    # Remove words inside square brackets\n",
    "    #text = re.sub(r'\\[\\w+\\]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    # Lemmatize each word\n",
    "    #lemmatized_text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "    lemmatized_text = ' '.join(lemmatizer.lemmatize(word) for word in words)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|███████████████████████████████████████████████████████████████████████████| 21072/21072 [00:56<00:00, 373.25it/s]"
     ]
    }
   ],
   "source": [
    "df['complaint_what_happened_lemma'] = df['complaint_what_happened_cleaned'].progress_apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint_what_happened_cleaned</th>\n",
       "      <th>complaint_what_happened_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good morning my name is xxxx xxxx and i appreciate it if you could help me put a stop to chase b...</td>\n",
       "      <td>good morning my name is xxxx xxxx and i appreciate it if you could help me put a stop to chase b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i upgraded my xxxx xxxx card in xx/xx/2018 and was told by the agent who did the upgrade my anni...</td>\n",
       "      <td>i upgraded my xxxx xxxx card in xx/xx/2018 and wa told by the agent who did the upgrade my anniv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>chase card was reported on xx/xx/2019  however  fraudulent application have been submitted my id...</td>\n",
       "      <td>chase card wa reported on xx/xx/2019 however fraudulent application have been submitted my ident...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>on xx/xx/2018  while trying to book a xxxx  xxxx  ticket  i came across an offer for   300 00  t...</td>\n",
       "      <td>on xx/xx/2018 while trying to book a xxxx xxxx ticket i came across an offer for 300 00 to be ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>my grand son give me check for   1600 00  i deposit it into my chase account after fund clear my...</td>\n",
       "      <td>my grand son give me check for 1600 00 i deposit it into my chase account after fund clear my ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78303</th>\n",
       "      <td>after being a chase card customer for well over a decade  was offered multiple solicitations for...</td>\n",
       "      <td>after being a chase card customer for well over a decade wa offered multiple solicitation for ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78309</th>\n",
       "      <td>on wednesday  xx/xx/xxxx i called chas  my xxxx xxxx visa credit card provider  and asked how to...</td>\n",
       "      <td>on wednesday xx/xx/xxxx i called chas my xxxx xxxx visa credit card provider and asked how to ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78310</th>\n",
       "      <td>i am not familiar with xxxx pay and did not understand the great risk this provides to consumers...</td>\n",
       "      <td>i am not familiar with xxxx pay and did not understand the great risk this provides to consumer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78311</th>\n",
       "      <td>i have had flawless credit for 30 yrs  i ve had chase credit cards    chase freedom    specifica...</td>\n",
       "      <td>i have had flawless credit for 30 yr i ve had chase credit card chase freedom specifically since...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78312</th>\n",
       "      <td>roughly 10  years ago i closed out my accounts with jp morgan chase bank xxxx in order to close ...</td>\n",
       "      <td>roughly 10 year ago i closed out my account with jp morgan chase bank xxxx in order to close out...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21072 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           complaint_what_happened_cleaned  \\\n",
       "1      good morning my name is xxxx xxxx and i appreciate it if you could help me put a stop to chase b...   \n",
       "2      i upgraded my xxxx xxxx card in xx/xx/2018 and was told by the agent who did the upgrade my anni...   \n",
       "10     chase card was reported on xx/xx/2019  however  fraudulent application have been submitted my id...   \n",
       "11     on xx/xx/2018  while trying to book a xxxx  xxxx  ticket  i came across an offer for   300 00  t...   \n",
       "14     my grand son give me check for   1600 00  i deposit it into my chase account after fund clear my...   \n",
       "...                                                                                                    ...   \n",
       "78303  after being a chase card customer for well over a decade  was offered multiple solicitations for...   \n",
       "78309  on wednesday  xx/xx/xxxx i called chas  my xxxx xxxx visa credit card provider  and asked how to...   \n",
       "78310  i am not familiar with xxxx pay and did not understand the great risk this provides to consumers...   \n",
       "78311  i have had flawless credit for 30 yrs  i ve had chase credit cards    chase freedom    specifica...   \n",
       "78312  roughly 10  years ago i closed out my accounts with jp morgan chase bank xxxx in order to close ...   \n",
       "\n",
       "                                                                             complaint_what_happened_lemma  \n",
       "1      good morning my name is xxxx xxxx and i appreciate it if you could help me put a stop to chase b...  \n",
       "2      i upgraded my xxxx xxxx card in xx/xx/2018 and wa told by the agent who did the upgrade my anniv...  \n",
       "10     chase card wa reported on xx/xx/2019 however fraudulent application have been submitted my ident...  \n",
       "11     on xx/xx/2018 while trying to book a xxxx xxxx ticket i came across an offer for 300 00 to be ap...  \n",
       "14     my grand son give me check for 1600 00 i deposit it into my chase account after fund clear my ch...  \n",
       "...                                                                                                    ...  \n",
       "78303  after being a chase card customer for well over a decade wa offered multiple solicitation for ac...  \n",
       "78309  on wednesday xx/xx/xxxx i called chas my xxxx xxxx visa credit card provider and asked how to ma...  \n",
       "78310  i am not familiar with xxxx pay and did not understand the great risk this provides to consumer ...  \n",
       "78311  i have had flawless credit for 30 yr i ve had chase credit card chase freedom specifically since...  \n",
       "78312  roughly 10 year ago i closed out my account with jp morgan chase bank xxxx in order to close out...  \n",
       "\n",
       "[21072 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['complaint_what_happened_cleaned','complaint_what_happened_lemma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "uXnN7aa_JrF0"
   },
   "outputs": [],
   "source": [
    "#Create a dataframe('df_clean') that will have only the complaints and the lemmatized complaints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "nOiDVvEIJrF0"
   },
   "outputs": [],
   "source": [
    "df_clean = df[['complaint_what_happened_cleaned','complaint_what_happened_lemma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = df_clean['complaint_what_happened_lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef pos_tag(text):\\n  # write your code here\\n\\n\\n\\ndf_clean[\"complaint_POS_removed\"] =  #this column should contain lemmatized text with all the words removed which have tags other than NN[tag == \"NN\"].\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write your function to extract the POS tags\n",
    "'''\n",
    "def pos_tag(text):\n",
    "  # write your code here\n",
    "\n",
    "\n",
    "\n",
    "df_clean[\"complaint_POS_removed\"] =  #this column should contain lemmatized text with all the words removed which have tags other than NN[tag == \"NN\"].\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns_column(text):\n",
    "    words = word_tokenize(text)  # Tokenize text\n",
    "    tagged_words = pos_tag(words)  # Get POS tags\n",
    "    lemmatized_nouns = [lemmatizer.lemmatize(word) for word, tag in tagged_words if tag == 'NN']  # Keep only nouns\n",
    "    return ' '.join(lemmatized_nouns)  # Return as a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████████████████████▉                                             | 8733/21072 [01:47<02:31, 81.34it/s]"
     ]
    }
   ],
   "source": [
    "df['lemmatized_nouns'] = df['complaint_what_happened_lemma'].progress_apply(extract_nouns_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'lemmatized_nouns':'complaint_POS_removed'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['complaint_POS_removed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kk7fc4DuJrF1"
   },
   "outputs": [],
   "source": [
    "#Write your function to extract the POS tags\n",
    "'''\n",
    "def pos_tag(text):\n",
    "  # write your code here\n",
    "\n",
    "\n",
    "\n",
    "df_clean[\"complaint_POS_removed\"] =  #this column should contain lemmatized text with all the words removed which have tags other than NN[tag == \"NN\"].\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjxfchvFJrF2"
   },
   "outputs": [],
   "source": [
    "#The clean dataframe should now contain the raw complaint, lemmatized complaint and the complaint after removing POS tags.\n",
    "df_clean = df[['complaint_what_happened_cleaned','complaint_what_happened_lemma','complaint_POS_removed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7Un1AElJrF2"
   },
   "source": [
    "## Exploratory data analysis to get familiar with the data.\n",
    "\n",
    "Write the code in this task to perform the following:\n",
    "\n",
    "*   Visualise the data according to the 'Complaint' character length\n",
    "*   Using a word cloud find the top 40 words by frequency among all the articles after processing the text\n",
    "*   Find the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text. ‘\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-zaqJF6JrF2"
   },
   "outputs": [],
   "source": [
    "# Write your code here to visualise the data according to the 'Complaint' character length\n",
    "df_clean['Char_length'] = [len(x) for x in df_clean['complaint_what_happened_cleaned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data=df_clean,x=df_clean['Char_length'],bins=20)\n",
    "plt.title('Distribution of Character Length')\n",
    "plt.xlabel('Complaint Lenth')\n",
    "plt.ylabel('No Of Complaints')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9jD_6SeJrF3"
   },
   "source": [
    "#### Find the top 40 words by frequency among all the articles after processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#df_clean['complaint_what_happened_cleaned']\n",
    "all_text = ' '.join(df_clean['complaint_what_happened_cleaned'])\n",
    "words = all_text.split()\n",
    "word_counts = Counter(words)\n",
    "#word_counts = dict(word_counts)\n",
    "top_words = dict(word_counts.most_common(40))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(top_words)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Add 'xx/xx/xxxx' to stopwords\n",
    "custom_stopwords = STOPWORDS.union({'xx/xx/xxxx','xx','XXXX','x'})\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    stopwords=custom_stopwords,\n",
    "    #width=800, \n",
    "    #height=400, \n",
    "    background_color=\"white\",\n",
    "    max_font_size=38,\n",
    "    max_words=38,\n",
    "    random_state=42\n",
    ").generate_from_frequencies(top_words)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "stop_words = set(STOPWORDS)\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white',  # Change background color if needed\n",
    "    stopwords=stop_words,  # Ensure stop_words is defined or set to None\n",
    "    max_font_size=38,\n",
    "    max_words=38,\n",
    "    random_state=42\n",
    ").generate(\" \".join(df_clean['complaint_POS_removed']))  # Convert column to a single string\n",
    "\n",
    "# Display the Word Cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = STOPWORDS.union({'xx/xx/xxxx','xx','xxxx'})\n",
    "word_cloud = WordCloud(\n",
    "    background_color='white',  # Change background color if needed\n",
    "    stopwords=custom_stopwords,  # Ensure stop_words is defined or set to None\n",
    "    max_font_size=38,\n",
    "    max_words=38,\n",
    "    random_state=42\n",
    ").generate(\" \".join(df_clean['complaint_POS_removed']))  # Convert column to a single string\n",
    "\n",
    "# Display the Word Cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "stop_words = set(STOPWORDS)\n",
    "word_cloud = WordCloud(\n",
    "                          background_color='green',\n",
    "                          stopwords=stop_words,\n",
    "                          max_font_size=38,\n",
    "                          max_words=38,\n",
    "                          random_state=42\n",
    "                         ).generate(str(df_clean['complaint_POS_removed']))\n",
    "\n",
    "fig = plt.figure(figsize=(20,16))\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcfdvtfZJrF3"
   },
   "outputs": [],
   "source": [
    "#Using a word cloud find the top 40 words by frequency among all the articles after processing the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkSmc3UaJrF4"
   },
   "outputs": [],
   "source": [
    "#Removing -PRON- from the text corpus\n",
    "df_clean['Complaint_clean'] = df_clean['complaint_POS_removed'].str.replace('-PRON-', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DfCSbbmJrF4"
   },
   "source": [
    "#### Find the top unigrams,bigrams and trigrams by frequency among all the complaints after processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mbk5DS5JrF4"
   },
   "outputs": [],
   "source": [
    "#Write your code here to find the top 30 unigram frequency among the complaints in the cleaned datafram(df_clean).\n",
    "df_clean['unigrams'] = df_clean['Complaint_clean'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unigrams = [word for words_list in df_clean['unigrams'] for word in words_list]\n",
    "unigram_freq = Counter(all_unigrams)\n",
    "# Get the top 30 unigrams\n",
    "top_30_unigrams = unigram_freq.most_common(30)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "df_top_unigrams = pd.DataFrame(top_30_unigrams, columns=['Unigram', 'Frequency'])\n",
    "\n",
    "# Display the result\n",
    "print(df_top_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Download stopwords if not already downloaded\n",
    "#nltk.download('stopwords')\n",
    "# Load default English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add custom stopwords\n",
    "custom_stopwords = {'xx', 'xxxx', 'xx/xx/xxxx', '00/00/0000', 'dummyword'}  # Add your custom words here\n",
    "stop_words.update(custom_stopwords)  # Merge both sets\n",
    "\n",
    "# Extract unigrams from each complaint and remove stopwords\n",
    "df_clean['unigrams'] = df_clean['Complaint_clean'].progress_apply(\n",
    "    lambda x: [word for word in word_tokenize(x) if word.lower() not in stop_words]\n",
    ")\n",
    "\n",
    "# Flatten the list of all unigrams across complaints\n",
    "all_unigrams = [word for words_list in df_clean['unigrams'] for word in words_list]\n",
    "\n",
    "# Count the frequency of each unigram\n",
    "unigram_freq = Counter(all_unigrams)\n",
    "\n",
    "# Get the top 30 unigrams\n",
    "top_30_unigrams = unigram_freq.most_common(30)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "df_top_unigrams = pd.DataFrame(top_30_unigrams, columns=['Unigram', 'Frequency'])\n",
    "\n",
    "# Display the result\n",
    "print(df_top_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#from collections import Counter\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.util import bigrams, trigrams\n",
    "#import nltk\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Load default English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add custom stopwords\n",
    "custom_stopwords = {'XX', 'XXXX', 'xx/xx/xxxx', '00/00/0000', 'dummyword'}\n",
    "stop_words.update(custom_stopwords)  # Merge both sets\n",
    "\n",
    "# Function to generate bigrams and trigrams\n",
    "def get_ngrams(text, ngram_type='bigram'):\n",
    "    words = [word for word in word_tokenize(text) if word.lower() not in stop_words]\n",
    "    if ngram_type == 'bigram':\n",
    "        return list(bigrams(words))  # Generate bigrams\n",
    "    elif ngram_type == 'trigram':\n",
    "        return list(trigrams(words))  # Generate trigrams\n",
    "    return []\n",
    "\n",
    "# Extract bigrams and trigrams from each complaint\n",
    "df_clean['bigrams'] = df_clean['Complaint_clean'].apply(lambda x: get_ngrams(x, 'bigram'))\n",
    "df_clean['trigrams'] = df_clean['Complaint_clean'].apply(lambda x: get_ngrams(x, 'trigram'))\n",
    "\n",
    "# Flatten the list of bigrams and trigrams across all complaints\n",
    "all_bigrams = [pair for pairs_list in df_clean['bigrams'] for pair in pairs_list]\n",
    "all_trigrams = [triplet for triplets_list in df_clean['trigrams'] for triplet in triplets_list]\n",
    "\n",
    "# Count the frequency of each bigram and trigram\n",
    "bigram_freq = Counter(all_bigrams)\n",
    "trigram_freq = Counter(all_trigrams)\n",
    "\n",
    "# Get the top 30 bigrams and trigrams\n",
    "top_30_bigrams = bigram_freq.most_common(30)\n",
    "top_30_trigrams = trigram_freq.most_common(30)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "df_top_bigrams = pd.DataFrame(top_30_bigrams, columns=['Bigram', 'Frequency'])\n",
    "df_top_trigrams = pd.DataFrame(top_30_trigrams, columns=['Trigram', 'Frequency'])\n",
    "\n",
    "# Display the top bigrams and trigrams\n",
    "print(\"Top 30 Bigrams:\\n\", df_top_bigrams)\n",
    "print(\"\\nTop 30 Trigrams:\\n\", df_top_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YX7fedm1JrF8"
   },
   "outputs": [],
   "source": [
    "#Print the top 10 words in the unigram frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aV7kD7w8JrF8"
   },
   "outputs": [],
   "source": [
    "#Write your code here to find the top 30 bigram frequency among the complaints in the cleaned datafram(df_clean).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPnMNIpyJrF9"
   },
   "outputs": [],
   "source": [
    "#Print the top 10 words in the bigram frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xkh7vtbtJrF-"
   },
   "outputs": [],
   "source": [
    "#Write your code here to find the top 30 trigram frequency among the complaints in the cleaned datafram(df_clean).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REcVxNfvJrF-"
   },
   "outputs": [],
   "source": [
    "#Print the top 10 words in the trigram frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUXzFji0JrF_"
   },
   "source": [
    "## The personal details of customer has been masked in the dataset with xxxx. Let's remove the masked text as this will be of no use for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKda-a_IJrF_"
   },
   "outputs": [],
   "source": [
    "df_clean['Complaint_clean'] = df_clean['Complaint_clean'].str.replace('xxxx','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UIFk8fQJrF_"
   },
   "outputs": [],
   "source": [
    "#All masked texts has been removed\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-I0k0QtJrGA"
   },
   "source": [
    "## Feature Extraction\n",
    "Convert the raw texts to a matrix of TF-IDF features\n",
    "\n",
    "**max_df** is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\"\n",
    "max_df = 0.95 means \"ignore terms that appear in more than 95% of the complaints\"\n",
    "\n",
    "**min_df** is used for removing terms that appear too infrequently\n",
    "min_df = 2 means \"ignore terms that appear in less than 2 complaints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y8fGwaCPJrGA"
   },
   "outputs": [],
   "source": [
    "#Write your code here to initialise the TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_df=0.95,min_df=2,stop_words='english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYzD85nTJrGA"
   },
   "source": [
    "#### Create a document term matrix using fit_transform\n",
    "\n",
    "The contents of a document term matrix are tuples of (complaint_id,token_id) tf-idf score:\n",
    "The tuples that are not there have a tf-idf score of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffzdDpp_JrGB"
   },
   "outputs": [],
   "source": [
    "#Write your code here to create the Document Term Matrix by transforming the complaints column present in df_clean.\n",
    "dtm = tfidf.fit_transform(df_clean['Complaint_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tfidf_model)\n",
    "print(tfidf_model.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer.get_feature_names_out()[:10]\n",
    "pd.DataFrame(dtm.toarray(),columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Q9lwvNEJrGB"
   },
   "source": [
    "## Topic Modelling using NMF\n",
    "\n",
    "Non-Negative Matrix Factorization (NMF) is an unsupervised technique so there are no labeling of topics that the model will be trained on. The way it works is that, NMF decomposes (or factorizes) high-dimensional vectors into a lower-dimensional representation. These lower-dimensional vectors are non-negative which also means their coefficients are non-negative.\n",
    "\n",
    "In this task you have to perform the following:\n",
    "\n",
    "* Find the best number of clusters\n",
    "* Apply the best number to create word clusters\n",
    "* Inspect & validate the correction of each cluster wrt the complaints\n",
    "* Correct the labels if needed\n",
    "* Map the clusters to topics/cluster names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amLT4omWJrGB"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wYR1xUTJrGD"
   },
   "source": [
    "## Manual Topic Modeling\n",
    "You need to do take the trial & error approach to find the best num of topics for your NMF model.\n",
    "\n",
    "The only parameter that is required is the number of components i.e. the number of topics we want. This is the most crucial step in the whole topic modeling process and will greatly affect how good your final topics are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgd2A6bhJrGD"
   },
   "outputs": [],
   "source": [
    "#Load your nmf_model with the n_components i.e 5\n",
    "num_topics = 5 #write the value you want to test out\n",
    "\n",
    "#keep the random_state =40\n",
    "nmf_model = NMF(n_components=num_topics,random_state=40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPMDYbt_JrGE"
   },
   "outputs": [],
   "source": [
    "W = nmf_model.fit_transform(dtm)  # Document-topic matrix\n",
    "H = nmf_model.components_       # Topic-term matrix\n",
    "len(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.array(tfidf.get_feature_names_out())\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "topic_words = pd.DataFrame(np.zeros((num_topics,15)),\n",
    "             index=[f'Topic {i + 1}' for i in range(num_topics)],\n",
    "             columns=[f'Word {i + 1}' for i in range(15)]).astype(str)\n",
    "\n",
    "for i in range(num_topics):\n",
    "    ix = H[i].argsort()[::-1][:15]\n",
    "    topic_words.iloc[i] = words[ix]\n",
    "\n",
    "topic_words\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the Top15 words for each of the topics\n",
    "\n",
    "topic_words = pd.DataFrame(np.zeros((15,num_topics)),\n",
    "             #columns=[f'Topic {i + 1}' for i in range(num_topics)],\n",
    "             columns=[f'{i + 1}' for i in range(num_topics)],\n",
    "             index=[f'Word {i + 1}' for i in range(15)]).astype(str)\n",
    "\n",
    "for i in range(num_topics):\n",
    "    ix = H[i].argsort()[::-1][:15]\n",
    "    topic_words[f'{i + 1}'] = words[ix]\n",
    "\n",
    "topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "16kRfat5JrGE"
   },
   "outputs": [],
   "source": [
    "#Print the Top15 words for each of the topics\n",
    "'''\n",
    "words = np.array(vectorizer.get_feature_names_out)\n",
    "topic_words = pd.DataFrame(np.zeros((num_topics, 10)), index=[f'Topic {i + 1}' for i in range(num_topics)],\n",
    "                           columns=[f'Word {i + 1}' for i in range(10)]).astype(str)\n",
    "for i in range(num_topics):\n",
    "    ix = H[i].argsort()[::-1][:10]\n",
    "    topic_words.iloc[i] = words[ix]\n",
    "\n",
    "topic_words\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n_topics = 5  # Define number of topics\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)  # Initialize NMF\n",
    "W = nmf_model.fit_transform(tfidf_model)  # Document-topic matrix\n",
    "H = nmf_model.components_  # Topic-word matrix\n",
    "\n",
    "# Step 3: Extract Top 15 Words for Each Topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "topics_dict = {}  # Dictionary to store topic words\n",
    "\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-16:-1]]  # Get top 15 words\n",
    "    topics_dict[f\"Topic {topic_idx+1}\"] = top_words\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df_topics = pd.DataFrame(topics_dict)\n",
    "\n",
    "df_topics\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OIT7LmFJrGF"
   },
   "outputs": [],
   "source": [
    "#Create the best topic for each complaint in terms of integer value 0,1,2,3 & 4\n",
    "#W_df = pd.DataFrame(W, columns=[f'{i + 1}' for i in range(num_topics)])\n",
    "#W_df['max_topic'] = W_df.apply(lambda x: x.idxmax(), axis=1)\n",
    "#W_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peyYv-ORJrGF"
   },
   "outputs": [],
   "source": [
    "#Assign the best topic to each of the cmplaints in Topic Column\n",
    "df_clean['Topic'] = np.argmax(W,axis=1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[['complaint_what_happened_cleaned','Complaint_clean','Topic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQKpufSPJrGG"
   },
   "outputs": [],
   "source": [
    "#Print the first 5 Complaint for each of the Topics\n",
    "#df_clean=df_clean.groupby('Topic').head(5)\n",
    "#df_clean.sort_values('Topic')\n",
    "\n",
    "df_clean[['complaint_what_happened_cleaned','Topic']].groupby('Topic').head(5).sort_values('Topic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piyLxzj6v07j"
   },
   "source": [
    "#### After evaluating the mapping, if the topics assigned are correct then assign these names to the relevant topic:\n",
    "* Bank Account services\n",
    "* Credit card or prepaid card\n",
    "* Theft/Dispute Reporting\n",
    "* Mortgage/Loan\n",
    "* Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Create the dictionary of Topic names and Topics\n",
    "Topic_names = {0: 'Bank Account services',\n",
    "    1: 'Credit card or prepaid card',\n",
    "    2: 'Others',\n",
    "    3: 'Theft/Dispute Reporting',\n",
    "    4: 'Mortgage/Loan'   }\n",
    "#Replace Topics with Topic Names\n",
    "df_clean['Topic'] = df_clean['Topic'].map(Topic_names)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWpwDG4RJrGG"
   },
   "outputs": [],
   "source": [
    "#Create the dictionary of Topic names and Topics\n",
    "\n",
    "Topic_names = {1: 'Bank Account services',\n",
    "    2: 'Credit card or prepaid card',\n",
    "    3: 'Others',\n",
    "    4: 'Theft/Dispute Reporting',\n",
    "    5: 'Mortgage/Loan'   }\n",
    "#Replace Topics with Topic Names\n",
    "df_clean['Topic'] = df_clean['Topic'].map(Topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-2ULY5K6JrGG"
   },
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df_clean,y=df_clean['Topic_Categorey'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Mu0QBOcJrGH"
   },
   "source": [
    "## Supervised model to predict any new complaints to the relevant Topics.\n",
    "\n",
    "You have now build the model to create the topics for each complaints.Now in the below section you will use them to classify any new complaints.\n",
    "\n",
    "Since you will be using supervised learning technique we have to convert the topic names to numbers(numpy arrays only understand numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.rename(columns={'Topic':'Topic_Categorey'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_U8J3J8wJrGH"
   },
   "outputs": [],
   "source": [
    "#Create the dictionary again of Topic names and Topics\n",
    "Topic_Ids = {'Bank Account services' :1,\n",
    "    'Credit card or prepaid card':2,\n",
    "    'Others':3,\n",
    "    'Theft/Dispute Reporting':4,\n",
    "    'Mortgage/Loan':5    }\n",
    "#Replace Topics with Topic Names\n",
    "df_clean['Topic'] = df_clean['Topic_Categorey'].map(Topic_Ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWIgJUkQJrGH"
   },
   "outputs": [],
   "source": [
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xx-FrbkWJrGH"
   },
   "outputs": [],
   "source": [
    "#Keep the columns\"complaint_what_happened\" & \"Topic\" only in the new dataframe --> training_data\n",
    "training_data=df_clean[['complaint_what_happened_cleaned','Topic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVg2pa12JrGI"
   },
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "280Vbqk-7a8M"
   },
   "source": [
    "####Apply the supervised models on the training data created. In this process, you have to do the following:\n",
    "* Create the vector counts using Count Vectoriser\n",
    "* Transform the word vecotr to tf-idf\n",
    "* Create the train & test data using the train_test_split on the tf-idf & topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUlQpgkzJrGI"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Write your code to get the Vector count\n",
    "\n",
    "\n",
    "#Write your code here to transform the word vector to tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMU3vj6w-wqL"
   },
   "source": [
    "You have to try atleast 3 models on the train & test data from these options:\n",
    "* Logistic regression\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Naive Bayes (optional)\n",
    "\n",
    "**Using the required evaluation metrics judge the tried models and select the ones performing the best**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udLHpPsZJrGI"
   },
   "outputs": [],
   "source": [
    "# Write your code here to build any 3 models and evaluate them using the required metrics\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2OznsObJrGP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "T9jD_6SeJrF3",
    "5DfCSbbmJrF4",
    "yYzD85nTJrGA",
    "piyLxzj6v07j",
    "280Vbqk-7a8M"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
